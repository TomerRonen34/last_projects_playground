{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dump_dir = Path(\"mbart50_dumps\")\n",
    "\n",
    "all_bt_results = {}\n",
    "for dump_path in dump_dir.iterdir():\n",
    "    # if \"backtranslation\" in dump_path.name and \"just_one\" in dump_path.name:\n",
    "    # if \"backtranslation\" in dump_path.name and \"just_one\" not in dump_path.name and \"en-\" in dump_path.name:\n",
    "    # if \"backtranslation\" in dump_path.name and \"-en\" in dump_path.name:\n",
    "    if \"backtranslation\" in dump_path.name:\n",
    "        with jsonlines.open(dump_path, 'r') as reader:\n",
    "            bt_results = list(reader)\n",
    "        bt_results = pd.DataFrame(bt_results)\n",
    "        \n",
    "        score_cols = [col for col in bt_results.columns if \"score\" in col and \"bt\" not in col]\n",
    "        bt_score_cols = [col for col in bt_results.columns if (\"score\" in col or \"logprob\" in col) and \"bt\" in col]\n",
    "\n",
    "        all_reranked_scores = []\n",
    "        for rerank_column in bt_score_cols:\n",
    "            idxmax = bt_results.groupby(\"id\")[rerank_column].idxmax()\n",
    "            bt_results_reranked = bt_results.loc[idxmax]\n",
    "            reranked_scores = bt_results_reranked[score_cols].mean()\n",
    "            reranked_scores.name = f\"reranked_{rerank_column}\"\n",
    "            all_reranked_scores.append(reranked_scores)\n",
    "        all_reranked_scores = pd.concat(all_reranked_scores, axis=1)\n",
    "        best_reranked_scores = all_reranked_scores.T.max()\n",
    "        best_reranked_scores.name = \"reranked_best\"\n",
    "\n",
    "        bt_results_ranked_by_confidence = bt_results.drop_duplicates(\"id\", keep=\"first\")\n",
    "        orig_scores = bt_results_ranked_by_confidence[score_cols].mean()\n",
    "        orig_scores.name = \"orig\"\n",
    "\n",
    "        oracle_best_scores = {}\n",
    "        oracle_worst_scores = {}\n",
    "        for score_col in score_cols:\n",
    "            sorted = bt_results[[\"id\", score_col]].sort_values([\"id\", score_col])\n",
    "            oracle_best = sorted.drop_duplicates(\"id\", keep=\"last\")[score_col].mean()\n",
    "            oracle_worst = sorted.drop_duplicates(\"id\", keep=\"first\")[score_col].mean()\n",
    "            oracle_best_scores[score_col] = oracle_best\n",
    "            oracle_worst_scores[score_col] = oracle_worst\n",
    "        oracle_best_scores = pd.DataFrame({\"oracle_best\": oracle_best_scores})\n",
    "        oracle_worst_scores = pd.DataFrame({\"oracle_worst\": oracle_worst_scores})\n",
    "\n",
    "        random_ranking_scores = bt_results[score_cols].mean()\n",
    "        random_ranking_scores.name = \"random\"\n",
    "\n",
    "        is_better = best_reranked_scores > orig_scores\n",
    "        is_better.name = \"is_better\"\n",
    "\n",
    "        all_scores = pd.concat([orig_scores, best_reranked_scores, is_better, oracle_best_scores, random_ranking_scores, oracle_worst_scores, all_reranked_scores], axis=1)\n",
    "\n",
    "        lang_pair = [substr for substr in dump_path.name.split('_') if '-' in substr][0]\n",
    "\n",
    "        all_bt_results[lang_pair] = bt_results\n",
    "        \n",
    "        print()\n",
    "        print(lang_pair)\n",
    "        display(all_scores)\n",
    "\n",
    "\n",
    "#TODO: why so low? Doesn't match paper (https://arxiv.org/pdf/2008.00401.pdf, last page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.5\n",
    "train_df, val_df = [], []\n",
    "for lang_pair, lang_df in all_bt_results.items():\n",
    "    lang_df[\"lang_pair\"] = lang_pair\n",
    "    ids = lang_df[\"id\"].unique()\n",
    "    train_ids, val_ids = train_test_split(ids, test_size=test_size, random_state=1)\n",
    "    is_train = lang_df[\"id\"].isin(train_ids)\n",
    "    curr_train_df = lang_df.loc[is_train]\n",
    "    curr_val_df = lang_df.loc[~is_train]\n",
    "    assert len(set(curr_train_df[\"id\"]).intersection(set(curr_val_df[\"id\"]))) == 0\n",
    "    train_df.append(curr_train_df)\n",
    "    val_df.append(curr_val_df)\n",
    "train_df = pd.concat(train_df, ignore_index=True)\n",
    "val_df = pd.concat(val_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "is_pairwise = True\n",
    "\n",
    "feature_names = [\"gen_logprob\"] + [col for col in bt_results.columns if (\"score\" in col or \"logprob\" in col) and \"bt\" in col]\n",
    "target_column = \"bertscore_f1\"\n",
    "\n",
    "def merge_df(df):\n",
    "    df = df[[\"lang_pair\", \"id\"] + feature_names + [target_column]].reset_index()\n",
    "    df_merged = df.merge(df, on=[\"lang_pair\", \"id\"])\n",
    "    df_merged = df_merged.loc[df_merged[\"index_x\"] != df_merged[\"index_y\"]]\n",
    "\n",
    "    target_diff = df_merged[f\"{target_column}_x\"] - df_merged[f\"{target_column}_y\"]\n",
    "    target_comparison = np.digitize(target_diff, [-0.05, 0.05])\n",
    "\n",
    "    df_merged[\"target_comparison\"] = target_comparison\n",
    "    return df_merged\n",
    "\n",
    "if is_pairwise:\n",
    "    train_pairwise_df = merge_df(train_df)\n",
    "    val_pairwise_df = merge_df(val_df)\n",
    "    print(pd.Series(train_pairwise_df[\"target_comparison\"] - 1).value_counts().sort_index())\n",
    "    print(pd.Series(val_pairwise_df[\"target_comparison\"] - 1).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
    "\n",
    "if is_pairwise:\n",
    "    feature_cols = [col for col in train_pairwise_df.columns if any([name in col for name in feature_names])]\n",
    "    train_X = train_pairwise_df[feature_cols]\n",
    "    train_y = train_pairwise_df[\"target_comparison\"]\n",
    "    val_X = val_pairwise_df[feature_cols]\n",
    "    val_y = val_pairwise_df[\"target_comparison\"]\n",
    "\n",
    "    # scaler_class = FunctionTransformer\n",
    "    scaler_class = RobustScaler\n",
    "    features_scaler = scaler_class().fit(train_X)\n",
    "\n",
    "    train_X = features_scaler.transform(train_X)\n",
    "    val_X = features_scaler.transform(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# model = LinearSVC(verbose=2)\n",
    "model = RandomForestClassifier(n_estimators=3, verbose=2, class_weight=\"balanced\")\n",
    "model.fit(train_X, train_y)\n",
    "train_pred = model.predict(train_X)\n",
    "val_pred = model.predict(val_X)\n",
    "\n",
    "print(classification_report(y_true=train_y, y_pred=train_pred))\n",
    "print(classification_report(y_true=val_y, y_pred=val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairwise_df[\"pairwise_pred\"] = train_pred\n",
    "val_pairwise_df[\"pairwise_pred\"] = val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_y = val_pairwise_df.groupby([\"lang_pair\", \"id\", \"index_x\"])[\"target_comparison\"].sum()\n",
    "sum_pred = val_pairwise_df.groupby([\"lang_pair\", \"id\", \"index_x\"])[\"pairwise_pred\"].sum()\n",
    "\n",
    "sum_y = sum_y.sort_values().reset_index(level=2)\n",
    "sum_pred = sum_pred.sort_values().reset_index(level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_best_indices = sum_y.loc[~sum_y.index.duplicated(keep=\"last\")][\"index_x\"]\n",
    "predicted_best_indices = sum_pred.loc[~sum_y.index.duplicated(keep=\"last\")][\"index_x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_best = val_df.loc[actual_best_indices].groupby([\"lang_pair\"])[\"bertscore_f1\"].mean()\n",
    "predicted_best = val_df.loc[predicted_best_indices].groupby([\"lang_pair\"])[\"bertscore_f1\"].mean()\n",
    "orig = val_df.sort_values([\"lang_pair\", \"id\", \"gen_logprob\"]\n",
    "                        ).drop_duplicates([\"lang_pair\", \"id\"], keep=\"last\").groupby([\"lang_pair\"])[\"bertscore_f1\"].mean()\n",
    "is_better = predicted_best > orig\n",
    "pd.DataFrame({\"actual_best\": actual_best, \"predicted_best\": predicted_best, \"orig\": orig, \"is_better\": is_better})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "if is_pairwise:\n",
    "    raise\n",
    "\n",
    "feature_names = [\"gen_logprob\"] + [col for col in bt_results.columns if (\"score\" in col or \"logprob\" in col) and \"bt\" in col]\n",
    "# feature_names = ['gen_logprob', 'bt_labels_logprob', 'bt_bertscore_f1']\n",
    "# feature_names = ['gen_logprob']\n",
    "target_column = \"bertscore_f1\"\n",
    "# target_column = \"bleu_score\"\n",
    "\n",
    "train_X = train_df[feature_names]\n",
    "train_y = train_df[target_column]\n",
    "val_X = val_df[feature_names]\n",
    "val_y = val_df[target_column]\n",
    "\n",
    "scaler_class = FunctionTransformer\n",
    "# scaler_class = RobustScaler\n",
    "features_scaler = scaler_class().fit(train_X)\n",
    "target_scaler = scaler_class().fit(pd.DataFrame(train_y))\n",
    "\n",
    "train_X = features_scaler.transform(train_X)\n",
    "train_y = target_scaler.transform(pd.DataFrame(train_y)).squeeze()\n",
    "val_X = features_scaler.transform(val_X)\n",
    "val_y = target_scaler.transform(pd.DataFrame(val_y)).squeeze()\n",
    "\n",
    "# model = linear_model.Lasso(alpha=0.001)\n",
    "# model = linear_model.Ridge(alpha=0.02)\n",
    "# model = linear_model.Ridge(alpha=0)\n",
    "model = RandomForestRegressor(n_estimators=5, max_depth=5, verbose=2)\n",
    "model.fit(train_X, train_y)\n",
    "train_pred = model.predict(train_X)\n",
    "val_pred = model.predict(val_X)\n",
    "train_mse = np.mean((train_pred - train_y)**2)\n",
    "val_mse = np.mean((val_pred - val_y)**2)\n",
    "\n",
    "print(\"std:\", train_pred.std(), val_pred.std())\n",
    "print(\"mse:\", train_mse, val_mse)\n",
    "if hasattr(model, \"coef_\"):\n",
    "    print(model.coef_)\n",
    "    print(np.array(feature_names)[model.coef_ != 0])\n",
    "\n",
    "val_df[\"model_score\"] = val_pred\n",
    "train_df[\"model_score\"] = train_pred\n",
    "\n",
    "def rerank(df, rerank_column):\n",
    "    return (df.sort_values([\"lang_pair\", \"id\", rerank_column])\n",
    "              .drop_duplicates([\"lang_pair\", \"id\"], keep=\"last\")\n",
    "              .groupby(\"lang_pair\")[score_cols].mean().T)\n",
    "\n",
    "rerank_val = rerank(val_df, \"model_score\")\n",
    "orig_val = rerank(val_df, \"gen_logprob\")\n",
    "oracle_val = rerank(val_df, \"bertscore_f1\")\n",
    "display(\"rerank_val > orig_val\", rerank_val > orig_val)\n",
    "display(\"rerank_val - orig_val\", rerank_val - orig_val)\n",
    "display(\"rerank_val\", rerank_val)\n",
    "display(\"orig_val\", orig_val)\n",
    "display(\"oracle_val\", oracle_val)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "rerank_train = rerank(train_df, \"model_score\")\n",
    "orig_train = rerank(train_df, \"gen_logprob\")\n",
    "oracle_train = rerank(train_df, \"bertscore_f1\")\n",
    "display(\"rerank_train > orig_train\", rerank_train > orig_train)\n",
    "display(\"rerank_train - orig_train\", rerank_train - orig_train)\n",
    "display(\"rerank_train\", rerank_train)\n",
    "display(\"orig_train\", orig_train)\n",
    "display(\"oracle_train\", oracle_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mlskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1388c48c1b848a0052718750b16c870cff087208ab0cafaf53720e4cd74eb1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
