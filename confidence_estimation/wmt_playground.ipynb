{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbart50_translate import LANG_CODE_TO_FAIRSEQ_FORMAT\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"data/wmt20\")\n",
    "src_lang = \"en\"\n",
    "available_languages = [tgt_lang for tgt_lang in LANG_CODE_TO_FAIRSEQ_FORMAT.keys()\n",
    "                       if (data_dir / f\"wmt20.{src_lang}-{tgt_lang}.src\").exists()]\n",
    "for tgt_lang in available_languages:\n",
    "    print(f'GPUS=1 run_with_slurm {tgt_lang} $(which run_python_script) -m mbart50_translate '\n",
    "          f'--num_examples=500 --data_dir=\"data/wmt20\" --dump_dir=\"mbart50_dumps\" '\n",
    "          f'--src_lang=\"en\" --tgt_lang=\"{tgt_lang}\" ')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\", src_lang=\"he_IL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2(\" בעבריצ משפט\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbart50_translate import dict_of_lists_to_list_of_dicts\n",
    "import torch\n",
    "LABEL_PAD = -100\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "batch = tokenizer([\"One sentence\", \"Here's another sentence\"])\n",
    "tokenizer.src_lang = \"he_IL\"\n",
    "batch[\"labels\"] = tokenizer([\"משפט אחד\", \"הנה עוד משפט\"])[\"input_ids\"]\n",
    "batch = dict_of_lists_to_list_of_dicts(batch)\n",
    "batch = collator(batch)\n",
    "input_ids, attention_mask, labels = [batch[k] for k in [\"input_ids\", \"attention_mask\", \"labels\"]]\n",
    "decoder_input_ids = torch.where(\n",
    "    labels != LABEL_PAD, labels, tokenizer.pad_token_id)\n",
    "start_of_generation_eos_column = tokenizer.eos_token_id * \\\n",
    "    labels.new_ones((labels.shape[0], 1))\n",
    "decoder_input_ids = torch.concat(\n",
    "    [start_of_generation_eos_column, decoder_input_ids], dim=1)\n",
    "decoder_attention_mask = (\n",
    "    decoder_input_ids != tokenizer.pad_token_id).int()\n",
    "\n",
    "label_pad_column = LABEL_PAD * labels.new_ones((labels.shape[0], 1))\n",
    "labels_for_loss = torch.concat(\n",
    "    [label_pad_column, labels[:, 1:], label_pad_column], dim=1)\n",
    "\n",
    "forward_just_labels = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "forward_all = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    " decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask,\n",
    " labels=labels_for_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_just_labels.logits.argmax(-1), forward_all.logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_just_labels.logits.argmax(-1), batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(forward_out, decoder_input_ids, labels):\n",
    "    logprobs = forward_out.logits.log_softmax(dim=-1)\n",
    "    # without eos that starts generation, without forced bos token\n",
    "    if labels is None:\n",
    "        labels = decoder_input_ids[:, 2:].unsqueeze(-1)\n",
    "        logprobs = logprobs[:, 1:-1]\n",
    "    else:\n",
    "        labels = labels[:,1:].unsqueeze(-1)\n",
    "        labels = torch.where(labels != -100, labels, 1)\n",
    "        # without forced bos token, without extra predicted token at the end\n",
    "        logprobs = logprobs[:, 1:]\n",
    "    labels_mask = (labels != tokenizer.pad_token_id)\n",
    "    label_logprobs = logprobs.gather(index=labels, dim=-1)\n",
    "    label_logprobs = torch.where(\n",
    "        labels_mask, label_logprobs, logprobs.new([0.])).squeeze(-1)\n",
    "    sequence_logprobs = label_logprobs.sum(\n",
    "        dim=-1) / labels_mask.squeeze(-1).sum(dim=-1)\n",
    "    manual_loss = -label_logprobs.sum() / labels_mask.sum()\n",
    "    return manual_loss, sequence_logprobs\n",
    "\n",
    "manual_loss_all, sequence_logprobs_all = calc_loss(forward_all, decoder_input_ids, None)\n",
    "manual_loss_just_labels, sequence_logprobs_just_labels = calc_loss(forward_just_labels, None, labels)\n",
    "manual_loss_all, forward_all.loss, manual_loss_just_labels, forward_just_labels.loss\n",
    "sequence_logprobs_all, sequence_logprobs_just_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache/\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mbart50.tokenization_mbart50_fast import FAIRSEQ_LANGUAGE_CODES\n",
    "LANG_CODE_TO_FAIRSEQ_FORMAT = {long_language_code[:2]: long_language_code for long_language_code in FAIRSEQ_LANGUAGE_CODES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_en = [\"The head of the United Nations says there is no military solution in Syria\", \"lol\"]\n",
    "\n",
    "num_beams = 1\n",
    "tgt_lang_code = \"he\"\n",
    "max_output_to_input_ratio = 1.2\n",
    "\n",
    "model_inputs = tokenizer(article_en, return_tensors=\"pt\", padding=True)\n",
    "batch_size, input_length = model_inputs[\"input_ids\"].shape\n",
    "\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[LANG_CODE_TO_FAIRSEQ_FORMAT[tgt_lang_code]]\n",
    "\n",
    "gen_output = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_beams,\n",
    "    max_new_tokens=int(max_output_to_input_ratio * input_length),\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "# print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "tokenizer.batch_decode(gen_output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids, attention_mask = model_inputs[\"input_ids\"], model_inputs[\"attention_mask\"]\n",
    "decoder_input_ids = gen_output.sequences\n",
    "decoder_attention_mask = (decoder_input_ids != tokenizer.pad_token_id).int()\n",
    "labels_for_loss = torch.concat([-100*torch.ones((batch_size,1), dtype=int),torch.where(decoder_input_ids != 1, decoder_input_ids, -100)[:,2:], -100*torch.ones((batch_size,1), dtype=int)], dim=-1)\n",
    "forward_out = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask,\n",
    "                    labels=labels_for_loss)\n",
    "\n",
    "logprobs = forward_out.logits.log_softmax(dim=-1)\n",
    "logprobs = logprobs[:,1:-1]  # without forced bos token, without extra predicted token at the end\n",
    "labels = decoder_input_ids[:,2:].unsqueeze(-1)  # without eos that starts generation, without forced bos token\n",
    "labels_mask = (labels != tokenizer.pad_token_id)\n",
    "label_logprobs = logprobs.gather(index=labels, dim=-1)\n",
    "label_logprobs = torch.where(labels_mask, label_logprobs, logprobs.new([0.])).squeeze(-1)\n",
    "sequence_prob = label_logprobs.sum(dim=-1) / labels_mask.squeeze(-1).sum(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbart50_translate import MBart50Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out.loss, label_logprobs.sum() / labels_mask.sum()\n",
    "# labels_for_loss, decoder_input_ids, forward_out.logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out2 = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids[:,1:], decoder_attention_mask=decoder_attention_mask[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_out2.logits.argmax(dim=-1), gen_output.sequences\n",
    "tokenizer.batch_decode(forward_out2.logits.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = forward_out.logits.log_softmax(dim=-1)\n",
    "logprobs = logprobs[:,1:-1]  # without forced bos token, without extra predicted token at the end\n",
    "labels = decoder_input_ids[:,2:].unsqueeze(-1)  # without eos that starts generation, without forced bos token\n",
    "labels_mask = (labels != tokenizer.pad_token_id)\n",
    "label_logprobs = logprobs.gather(index=labels, dim=-1)\n",
    "label_logprobs = torch.where(labels_mask, label_logprobs, logprobs.new([0.])).squeeze(-1)\n",
    "sequence_prob = label_logprobs.sum(dim=-1) / labels_mask.squeeze(-1).sum(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "LABEL_PAD = -100\n",
    "forward_out.logits.gather(dim=-1, index=decoder_input_ids.unsqueeze(-1))\n",
    "forward_out.logits[:,1:].argmax(dim=-1), decoder_input_ids[:,2:]\n",
    "label_pad_column = LABEL_PAD * decoder_input_ids.new_ones((batch_size,1))\n",
    "faux_labels = torch.concat([label_pad_column, decoder_input_ids[:, 2:], label_pad_column], dim=1)\n",
    "faux_labels = torch.where(faux_labels != tokenizer.pad_token_id, faux_labels, LABEL_PAD)\n",
    "forward_out.logits.argmax(dim=-1), faux_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list: list[list]) -> list:\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "import torch\n",
    "special_tokens = flatten([[toks] if isinstance(toks, str) else toks\n",
    "                          for toks in tokenizer.special_tokens_map.values()])\n",
    "special_token_ids = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "special_token_ids = torch.tensor(special_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = gen_output.sequences.view(batch_size, num_beams, -1)\n",
    "tokenizer.convert_ids_to_tokens(tokenizer(\"lol\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = gen_output.sequences.view(batch_size, num_beams, -1)\n",
    "sequences = sequences[:, :, 1:]  # drop the eos token that starts generation\n",
    "sequences = [[seq[seq != tokenizer.pad_token_id].tolist() for seq in beam] for beam in sequences]\n",
    "scores = gen_output.sequences_scores.view(batch_size, num_beams).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"src_sentence\": [\"מדובר בחתול נאה מאוד\", \"אלליי, איזו מרשתת!\"], \"id\": [\"a\", \"b\"]})\n",
    "# dataset = dataset.map(tokenizer_many_to_en, batched=True, input_columns=[\"src_sentence\"])\n",
    "# tokenizer.batch_decode(model_many_to_en.generate(input_ids=torch.tensor([dataset[1][\"input_ids\"]]),\n",
    "#                           attention_mask=torch.tensor([dataset[1][\"attention_mask\"]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache/\"\n",
    "dataset.with_format(columns=[\"src_sentence\"])[[1,0]][\"src_sentence\"][0]\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_of_lists_to_list_of_dicts(d: dict[list]) -> list[dict]:\n",
    "    return [dict(zip(d.keys(), vals)) for vals in zip(*d.values())]\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "dataset = dataset.map(tokenizer, input_columns=\"src_sentence\", batched=True)\n",
    "batch = dataset.with_format(columns=[\"input_ids\", \"attention_mask\"])[[1,0]]\n",
    "batch = dict_of_lists_to_list_of_dicts(batch)\n",
    "collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_item({\"src_sentence\": \"aaa\", \"id\": \"g\", \"input_ids\": [3,4,4], \"attention_mask\": [1,1,1]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "dataset = Dataset.from_dict({\"text\": [\"This is a setnence.\", \"How many woods are there in the woods?\"]})\n",
    "dataset = dataset.map(tokenizer, input_columns=\"text\")\n",
    "if not \"forced_bos_token_id\" in dataset.column_names:\n",
    "    dataset = dataset.add_column(\"forced_bos_token_id\", [tokenizer.lang_code_to_id[\"hi_IN\"]] * len(dataset))\n",
    "trainer_args = Seq2SeqTrainingArguments(output_dir='/tmp/lol', predict_with_generate=True)\n",
    "trainer = Seq2SeqTrainer(model, args=trainer_args, data_collator=DataCollatorForSeq2Seq(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "generation_kwargs = dict(forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"], length_penalty=1.0, num_beams=2, num_return_sequences=2)\n",
    "def custom_generate(*args, **kwargs):\n",
    "    num_beams = 2\n",
    "    kwargs = {**kwargs, **generation_kwargs}\n",
    "    generated_tokens = model.orig_generate(*args, **kwargs)\n",
    "    generated_tokens = torch.hstack([generated_tokens, -100 * torch.ones((generated_tokens.shape[0], 1), dtype=int)])\n",
    "    batch_size = generated_tokens.shape[0] // num_beams\n",
    "    generated_tokens = generated_tokens.reshape(batch_size, -1)\n",
    "    return generated_tokens\n",
    "model.generate = custom_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_preds = trainer.predict(dataset).predictions\n",
    "concatenated_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(gen_output[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_many_to_en = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "tokenizer_many_to_en.src_lang = LANG_CODE_TO_FAIRSEQ_FORMAT[\"he\"]\n",
    "tokenizer_many_to_en.convert_ids_to_tokens(tokenizer_many_to_en(\"זהו משפט בעברית.\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.repeat([\"a\",\"fff\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score\n",
    "from pathlib import Path\n",
    "bertscore_baseline_languages = [path.name for path in (Path(bert_score.__file__).parent / \"rescale_baseline\").iterdir()]\n",
    "bertscore_baseline_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache/\"\n",
    "from mbart50_translate import MBart50Translator\n",
    "runner = MBart50Translator(device=\"cpu\", num_examples=200, batch_size=2, data_dir=\"data/wmt20\", src_lang=\"en\", tgt_lang=\"ta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids=torch.tensor([[250004,     62,  75533,  13416,   4568,   6602,  14037,     99,  19713,\n",
    "          35389,  77987,  27941,      7,     23,     70,   7082,   1902,   2809,\n",
    "          61689,   5281,    111,  17688,    538,     10,   1192,    202,   1916,\n",
    "            707, 162753,    449,   2363,  44828,  26255,    645,     10,  14922,\n",
    "            111,  22759,   5369,      5,      2,      1,      1,      1,      1,\n",
    "              1],\n",
    "        [250004,   6300,   1177,     33,   7582,   3640,    136,  18982,  11075,\n",
    "              4,  12638,  15889, 125413,   1221,  27154,     67,   2363,   7175,\n",
    "              4,    678,  56480,   8035,  19667,     19,   8305,  40101,     10,\n",
    "          85727,   1118,    707,  72761, 233547,     20, 117934,     10,  15889,\n",
    "             28,  27591,    818,  12126,   7175,  21771,  32316,      7,      5,\n",
    "              2],\n",
    "        [250004,   1529,     25,      7,   7730,     47,    186,  37515,      4,\n",
    "           1284,    450,  22027,     25,     18,  16401,    398,   5792,   4989,\n",
    "             23,    903,   6712,      5,      2,      1,      1,      1,      1,\n",
    "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
    "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
    "              1],\n",
    "        [250004,  11853,      4,     10,  27150,    332,  74703,      7,  41896,\n",
    "             99,     10,  33816,   8752,      6,  92621,   9149,  10519,     10,\n",
    "              6,  44720,  53470,     53,  43613,  20016,     15,  22489,     73,\n",
    "            434,  54969,     83,  49726,     71,   1660, 107314,   4049,      4,\n",
    "         179493,     10,  57571,    384,      9,  38184,    194,      2,      1,\n",
    "              1]])\n",
    "attention_mask=torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
    "labels=torch.tensor([[250004,     62,  75533,  13416,   4568,   6602,  14037,     99,\n",
    "          19713,  35389,  77987,  27941,      7,     23,     70,   7082,   1902,\n",
    "           2809,  61689,   5281,    111,  17688,    538,     10,   1192,    202,\n",
    "           1916,    707, 162753,    449,   2363,  44828,  26255,    645,     10,\n",
    "          14922,    111,  22759,   5369,      5,      2,      1,      1,      1,\n",
    "              1,      1],\n",
    "        [250004,   6300,   1177,     33,   7582,   3640,    136,  18982,\n",
    "          11075,      4,  12638,  15889, 125413,   1221,  27154,     67,   2363,\n",
    "           7175,      4,    678,  56480,   8035,  19667,     19,   8305,  40101,\n",
    "             10,  85727,   1118,    707,  72761, 233547,     20, 117934,     10,\n",
    "          15889,     28,  27591,    818,  12126,   7175,  21771,  32316,      7,\n",
    "              5,      2],\n",
    "        [250004,   1529,     25,      7,   7730,     47,    186,  37515,\n",
    "              4,   1284,    450,  22027,     25,     18,  16401,    398,   5792,\n",
    "           4989,     23,    903,   6712,      5,      2,      1,      1,      1,\n",
    "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
    "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
    "              1,      1],\n",
    "        [250004,  11853,      4,     10,  27150,    332,  74703,      7,\n",
    "          41896,     99,     10,  33816,   8752,      6,  92621,   9149,  10519,\n",
    "             10,      6,  44720,  53470,     53,  43613,  20016,     15,  22489,\n",
    "             73,    434,  54969,     83,  49726,     71,   1660, 107314,   4049,\n",
    "              4, 179493,     10,  57571,    384,      9,  38184,    194,      2,\n",
    "              1,      1]])\n",
    "labels = torch.where(labels != 1, labels, -100)\n",
    "forward_args, forward_out = runner._calculate_target_logprobs(input_ids, attention_mask, labels)\n",
    "\n",
    "forward_args[\"labels\"], labels, forward_out.logits.argmax(-1)\n",
    "\n",
    "decoder_input_ids = torch.where(labels != LABEL_PAD, labels, tokenizer.pad_token_id)\n",
    "eos_column = tokenizer.eos_token_id * labels.new_ones((labels.shape[0], 1))\n",
    "decoder_input_ids = torch.concat([eos_column, decoder_input_ids], dim=1)  # eos token marks start of generation\n",
    "\n",
    "logprobs = forward_out.logits.log_softmax(dim=-1)\n",
    "logprobs = logprobs[:,1:-1]  # without forced bos token, without extra predicted token at the end\n",
    "labels = decoder_input_ids[:,2:].unsqueeze(-1)  # without eos that starts generation, without forced bos token\n",
    "labels_mask = (labels != tokenizer.pad_token_id)\n",
    "label_logprobs = logprobs.gather(index=labels, dim=-1)\n",
    "label_logprobs = torch.where(labels_mask, label_logprobs, logprobs.new([0.])).squeeze(-1)\n",
    "sequence_logprobs = label_logprobs.sum(dim=-1) / labels_mask.squeeze(-1).sum(dim=-1)\n",
    "manual_loss = -label_logprobs.sum() / labels_mask.sum()\n",
    "manual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(forward_out.logits.shape)\n",
    "forward_out.logits.log_softmax(-1)[0,1,62], label_logprobs\n",
    "labels.squeeze(), logprobs.argmax(-1), forward_args[\"labels\"], forward_out.logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner._run_metrics_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\"gen_sequence\": [250021, 94, 5216, 4, 819, 35, 38626, 22238, 9542, 4, 9309, 743, 4, 414, 1097, 129, 60927, 1730, 29, 11373, 2192, 23054, 1339, 20, 13398, 1266, 5, 2], \"gen_score\": -0.3600703179836273, \"gen_text\": \"«Я, как и многие другие люди, верю, что это повлияет на мирный процесс», - сказал он.\", \"src_sentence\": \"\\\"I, along with many other people, believe that it will affect the peace process,\\\" he said.\", \"tgt_sentence\": \"«Я, как и многие другие люди, убежден, что это скажется на мирном процессе», – заявил он.\", \"id\": 115, \"input_ids\": [250004, 44, 568, 4, 33233, 678, 5941, 3789, 3395, 4, 18822, 450, 442, 1221, 52490, 70, 88669, 9433, 4, 58, 764, 2804, 5, 2], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"bertscore_f1\": 0.8917006850242615, \"bertscore_precision\": 0.9027066230773926, \"bertscore_recall\": 0.8809599876403809, \"bleu_score\": 0.4372411072254181, \"rougeL_score\": 0.0, \"rouge2_score\": 0.0, \"rouge1_score\": 0.0}\n",
    "from utils import rouge, sacrebleu\n",
    "gen, tgt = example[\"gen_text\"], example[\"tgt_sentence\"]\n",
    "\n",
    "def remove_wat(s):\n",
    "    return s.replace('«', '').replace('»', '')\n",
    "\n",
    "gen, tgt = remove_wat(gen), remove_wat(tgt)\n",
    "\n",
    "rouge(pred=gen, label=tgt, rouge_key=\"rouge1\")\n",
    "# sacrebleu(pred=gen, label=tgt)\n",
    "gen, tgt\n",
    "x = 'Я, как и многие другие люди, верю, что это повлияет на мирный'\n",
    "y = 'Я, как и многие другие люди, убежден, что это скажется на мирном'\n",
    "rouge(pred=\"a a a a\", label=\"a a a b\", rouge_key=\"rouge1\")\n",
    "[c for c in list(x) if not c.isalnum()]\n",
    "import re\n",
    "gen, re.sub(r'\\W', ' ', gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_json(\"/home/olab/tomerronen1/git_repos/last_projects_playground/confidence_estimation/mbart50_dumps/wmt20_en-ru_200examples.jsonl\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(1000000).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache/\"\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_json(\"/home/olab/tomerronen1/git_repos/last_projects_playground/confidence_estimation/mbart50_dumps/wmt20_en-ru_200examples.jsonl\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "bertscore_model = BERTScorer(model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\", rescale_with_baseline=True, device=\"cpu\")\n",
    "bertscore_model._model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten(nested_list: list[list]) -> list:\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "num_beams = 2\n",
    "preds = []\n",
    "for pred in concatenated_preds:\n",
    "    different_beams = np.array_split(pred, np.flatnonzero(pred == -100) + 1)\n",
    "    different_beams = different_beams[:-1]  # last one is padding\n",
    "    for beam_pred in different_beams:\n",
    "        beam_pred = beam_pred[beam_pred != -100]\n",
    "        preds.append(beam_pred)\n",
    "\n",
    "tokenizer.batch_decode(preds, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.data_collator(dataset.to_list())\n",
    "# trainer.data_collator(dataset.to_dict(orient=\"list\"))\n",
    "batch = trainer.data_collator(dataset.to_pandas()[[\"input_ids\", \"attention_mask\"]].to_dict(orient=\"records\"))\n",
    "# trainer.data_collator([dataset[i] for i in range(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.model.generate(**batch)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(dataset)\n",
    "preds.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_en = \"The head of the United Nations says there is no military solution in Syria\"\n",
    "\n",
    "model_inputs = tokenizer(article_en, return_tensors=\"pt\")\n",
    "generation_params = {\"num_beams\": 5, \"length_penalty\": 1.0}\n",
    "\n",
    "# translate from English to Hindi\n",
    "generated_tokens = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "    **generation_params\n",
    ")\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => 'संयुक्त राष्ट्र के नेता कहते हैं कि सीरिया में कोई सैन्य समाधान नहीं है'\n",
    "\n",
    "# translate from English to Chinese\n",
    "generated_tokens = model.generate(\n",
    "    **model_inputs,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"],\n",
    "    **generation_params\n",
    ")\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => '联合国首脑说,叙利亚没有军事解决办法'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_many_to_en.generate(**tokenizer_many_to_en(\"אני חתול\", return_tensors=\"pt\"), forced_bos_token_id=250004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_many_to_en = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_many_to_en = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "tokenizer_many_to_en = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_hi = \"संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है\"\n",
    "article_ar = \"الأمين العام للأمم المتحدة يقول إنه لا يوجد حل عسكري في سوريا.\"\n",
    "\n",
    "# translate Hindi to English\n",
    "tokenizer_many_to_en.src_lang = \"hi_IN\"\n",
    "encoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\n",
    "generated_tokens = model_many_to_en.generate(**encoded_hi, **generation_params)\n",
    "print(tokenizer_many_to_en.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"The head of the UN says there is no military solution in Syria.\"\n",
    "\n",
    "# translate Arabic to English\n",
    "tokenizer_many_to_en.src_lang = \"ar_AR\"\n",
    "encoded_ar = tokenizer_many_to_en(article_ar, return_tensors=\"pt\")\n",
    "generated_tokens = model_many_to_en.generate(**encoded_ar, **generation_params)\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "# => \"The Secretary-General of the United Nations says there is no military solution in Syria.\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mlskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1388c48c1b848a0052718750b16c870cff087208ab0cafaf53720e4cd74eb1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
