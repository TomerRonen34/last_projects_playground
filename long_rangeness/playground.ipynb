{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/olab/tomerronen1/xdg_cache/huggingface/modules/datasets_modules/datasets/hotpot_qa/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5 (last modified on Thu Sep 22 14:56:17 2022) since it couldn't be found locally at /home/olab/tomerronen1/git_repos/last_projects_playground/long_rangeness/hotpot_qa/hotpot_qa.py, or remotely (FileNotFoundError).\n",
      "Reusing dataset hotpot_qa (/home/olab/tomerronen1/xdg_cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b615c47af824ef89be73caaca3e93a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache\"\n",
    "os.environ[\"AUTH_TOKEN\"] = \"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\"\n",
    "import datasets\n",
    "dataset_dict = datasets.load_dataset(\"hotpot_qa\", \"distractor\")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which genus of moth in the world's seventh-largest country contains only one species?\n",
      "Crambidae\n",
      "{'title': ['Indogrammodes', 'Indogrammodes', 'India', 'India'], 'sent_id': [0, 1, 0, 1]}\n",
      "Indogrammodes is a genus of moths of the Crambidae family.\n",
      " It contains only one species, Indogrammodes pectinicornalis, which is found in India.\n",
      "India, officially the Republic of India (\"Bhārat Gaṇarājya\"), is a country in South Asia.\n",
      " It is the seventh-largest country by area, the second-most populous country (with over 1.2 billion people), and the most populous democracy in the world.\n"
     ]
    }
   ],
   "source": [
    "example = dataset_dict[\"train\"][6]\n",
    "print(example[\"question\"])\n",
    "print(example[\"answer\"])\n",
    "print(example[\"supporting_facts\"])\n",
    "for title, sentence_id in zip(example[\"supporting_facts\"][\"title\"], example[\"supporting_facts\"][\"sent_id\"]):\n",
    "    paragraph_id = example[\"context\"][\"title\"].index(title)\n",
    "    print(example[\"context\"][\"sentences\"][paragraph_id][sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c2d13058e244dd98d34e8a16da5088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69ff73d797a4398a5a9a440a53a0888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were unexpected keys in the checkpoint model loaded: ['_lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "import sled\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "model_name = \"tau/bart-base-sled-govreport\"\n",
    "auth_token = \"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\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=auth_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f60e1f313e450680b18c1baac38d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cache = []\n",
    "\n",
    "def append_grad(grad):\n",
    "    cache.append(grad.detach().cpu().numpy())\n",
    "\n",
    "emb_tensor = model._underlying_model.model.shared.weight\n",
    "emb_tensor.register_hook(append_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: gov_report_summarization_dataset/document\n",
      "Reusing dataset gov_report_summarization_dataset (/home/olab/tomerronen1/xdg_cache/huggingface/datasets/ccdv___gov_report_summarization_dataset)/document/1.0.0/57ca3042de9c40c218cc94084cbc80a99a161036134bfc88112c57d251443590)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/govreport-summarization\", split=\"validation[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olab/tomerronen1/miniconda3/envs/mlskel/lib/python3.9/site-packages/sled/modeling_sled.py:693: UserWarning: prefix_length is missing from kwargs_tensor_keys (though expected for SLED prefix prepending)\n",
      "  warnings.warn(f'{k} is missing from kwargs_tensor_keys (though expected for SLED prefix prepending)')\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(dataset[:2][\"report\"], return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "gen_batch = model.generate(**batch, num_beams=1, max_length=30)\n",
    "decoder_attention_mask = (gen_batch != tokenizer.pad_token_id).long()\n",
    "model_output = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "                     decoder_input_ids=gen_batch, decoder_attention_mask=decoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_output.logits[:,:-2,:]  # without additional token [-1], without the token that replaces the forced EOS [-2].\n",
    "generated_tokens = gen_batch[:,1:-1]  # without the EOS that starts the generation, without the forced EOS at the end.\n",
    "manual_greedy_output = logits.argmax(-1)\n",
    "assert (manual_greedy_output == generated_tokens).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprobs = logits.log_softmax(dim=-1)  # mote correct, but then everything has gradient - is that good?\n",
    "logprobs = logits\n",
    "generated_logprobs = torch.gather(input=logprobs, dim=-1, index=generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "manual_greedy_generated_logprobs = logprobs.max(dim=-1).values\n",
    "assert (generated_logprobs == manual_greedy_generated_logprobs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     2,     5,     6,     7,     9,    10,    11,    13,\n",
       "          15,    16,    17,    19,    20,    27,    29,    31,    32,\n",
       "          36,    41,    43,    59,    71,    95,   104,   121,   135,\n",
       "         155,   176,   195,   223,   231,   246,   248,   260,   309,\n",
       "         315,   336,   398,   401,   467,   488,   501,   532,   586,\n",
       "         598,   675,   694,   746,   752,   811,   943,  1070,  1093,\n",
       "        1127,  1138,  1466,  1588,  1788,  1814,  1926,  1956,  2074,\n",
       "        2096,  2266,  2447,  2546,  2619,  2885,  2946,  3072,  3073,\n",
       "        3206,  3248,  3253,  3818,  3868,  4141,  4268,  4669,  4741,\n",
       "        5187,  5860,  6467,  6643,  6804,  7278,  7539,  8594,  8878,\n",
       "        8879,  9628, 10832, 11206, 13522, 16055, 17313, 17660, 18798,\n",
       "       22645, 23066, 27193, 29531, 30680, 34124, 35590, 37529, 38027,\n",
       "       39578, 39923])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_sequence_logprob = generated_logprobs.sum(dim=-1)\n",
    "\n",
    "cache[:] = []\n",
    "batch_size = entire_sequence_logprob.shape[0]\n",
    "for i in range(batch_size):\n",
    "    entire_sequence_logprob[i].backward(retain_graph=True)\n",
    "    model.zero_grad()\n",
    "\n",
    "cache[0].sum(axis=-1).nonzero()[0]\n",
    "# woops, I need multiple copies for each input token!\n",
    "# maybe I should fetch the embedding vectors from without BartModel (immediately after calling the embedding layer) and put hooks on them?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mlskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1388c48c1b848a0052718750b16c870cff087208ab0cafaf53720e4cd74eb1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
