{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/home/olab/tomerronen1/xdg_cache\"\n",
    "os.environ[\"AUTH_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sled\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoConfig\n",
    "device = \"cpu\"\n",
    "model_name = \"tau/bart-base-sled-govreport\"\n",
    "auth_token = \"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\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=auth_token)\n",
    "model.to(device)\n",
    "# from transformers.models.bart.modeling_bart import BartModel\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class GradCache:\n",
    "    def __init__(self):\n",
    "        self.cache = []\n",
    "\n",
    "    def __call__(self, grad: Tensor) -> None:\n",
    "        if (grad != 0).any():\n",
    "            grad = grad.cpu().detach().clone()\n",
    "            self.cache.append(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ccdv/govreport-summarization\", split=\"validation[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(dataset[5:8][\"report\"], return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "batch_size = input_ids.shape[0]\n",
    "num_input_tokens = input_ids.shape[1]\n",
    "gen_batch = model.generate(input_ids=input_ids, attention_mask=attention_mask, num_beams=1, max_length=30)\n",
    "decoder_attention_mask = (gen_batch != tokenizer.pad_token_id).long()\n",
    "print(tokenizer.batch_decode(gen_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing_factor = 0.05\n",
    "# smoothing_num_samples = 3\n",
    "smoothing_factor = 0\n",
    "smoothing_num_samples = 1\n",
    "embedding_layer = model._underlying_model.model.shared\n",
    "inputs_embeds = embedding_layer(input_ids)\n",
    "inputs_embeds = torch.repeat_interleave(inputs_embeds, repeats=smoothing_num_samples, dim=0)\n",
    "std_range = inputs_embeds.max(dim=-1, keepdims=True).values - inputs_embeds.min(dim=-1, keepdims=True).values\n",
    "noise = torch.normal(torch.zeros_like(inputs_embeds), torch.ones_like(inputs_embeds) * smoothing_factor * std_range)\n",
    "inputs_embeds = inputs_embeds + noise\n",
    "\n",
    "attention_mask = torch.repeat_interleave(attention_mask, repeats=smoothing_num_samples, dim=0)\n",
    "decoder_input_ids = torch.repeat_interleave(gen_batch, repeats=smoothing_num_samples, dim=0)\n",
    "decoder_attention_mask = torch.repeat_interleave(decoder_attention_mask, repeats=smoothing_num_samples, dim=0)\n",
    "embed_dim = inputs_embeds.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = inputs_embeds.unbind()\n",
    "caches = []\n",
    "for sequence_embeds in inputs_embeds:\n",
    "    grad_cache = GradCache()\n",
    "    sequence_embeds.register_hook(grad_cache)\n",
    "    caches.append(grad_cache)\n",
    "inputs_embeds = torch.stack(inputs_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n",
    "                     decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_output.logits[:,1:-2,:]  # without additional generated token [-1], without the token that replaces the forced EOS [-2].\n",
    "generated_tokens = gen_batch[:,2:-1]  # without the EOS+BOS that start the generation, without the forced EOS at the end.\n",
    "num_target_tokens = generated_tokens.shape[1]\n",
    "generated_tokens_no_repeats = generated_tokens.clone()\n",
    "generated_tokens = torch.repeat_interleave(generated_tokens, repeats=smoothing_num_samples, dim=0)\n",
    "\n",
    "if (smoothing_factor == 0) and (smoothing_num_samples == 1):\n",
    "    print(\"Checking\")\n",
    "    manual_greedy_output = logits.argmax(-1)\n",
    "    assert (manual_greedy_output == generated_tokens).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = logits.log_softmax(dim=-1)  # more correct, but then every word in the dictionary participates - is that good?\n",
    "# logprobs = logits\n",
    "generated_logprobs = torch.gather(input=logprobs, dim=-1, index=generated_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "if (smoothing_factor == 0) and (smoothing_num_samples == 1):\n",
    "    print(\"Checking\")\n",
    "    manual_greedy_generated_logprobs = logprobs.max(dim=-1).values\n",
    "    assert (generated_logprobs == manual_greedy_generated_logprobs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# i_target_token = 6\n",
    "# i_target_token = 7\n",
    "# i_target_token = \"entire_sequence\"\n",
    "i_target_token = \"all_individual_target_tokens\"\n",
    "if i_target_token == \"all_individual_target_tokens\":\n",
    "    for i_example in tqdm(range(generated_logprobs.shape[0])):\n",
    "        for i_token in range(generated_logprobs.shape[1]):\n",
    "            generated_logprobs[i_example][i_token].backward(retain_graph=True)\n",
    "            model.zero_grad()\n",
    "else:\n",
    "    if i_target_token == \"entire_sequence\":\n",
    "        logprobs_to_derivate = generated_logprobs.sum(dim=-1)\n",
    "        for gen_seq in tokenizer.batch_decode(generated_tokens):\n",
    "            print(gen_seq)\n",
    "    else:\n",
    "        logprobs_to_derivate = generated_logprobs[:,i_target_token]\n",
    "        print(tokenizer.convert_ids_to_tokens(generated_tokens[:,i_target_token]))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        logprobs_to_derivate[i].backward(retain_graph=True)\n",
    "        model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = torch.stack([torch.stack(grad_cache.cache) for grad_cache in caches]) # bsz * smooth_samples, target tokens, input tokens, embed dim\n",
    "grads = grads.view(batch_size, smoothing_num_samples, num_target_tokens, num_input_tokens, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_to_scalar_method = \"l1_norm\"\n",
    "grad_to_scalar_factor = embed_dim\n",
    "if grad_to_scalar_method == \"l1_norm\":\n",
    "    saliency = grads.abs().sum(dim=-1)\n",
    "elif grad_to_scalar_method == \"grad_dot_input\":\n",
    "    saliency = (grads * inputs_embeds.unsqueeze(1)).sum(dim=-1)\n",
    "elif grad_to_scalar_method == \"l2_norm\":\n",
    "    saliency = (grads ** 2).sum(dim=-1)\n",
    "else:\n",
    "    raise ValueError()\n",
    "saliency = saliency / grad_to_scalar_factor\n",
    "\n",
    "# saliency shape before smoothing: [bsz, smoothing_num_samples, target tokens, input tokens]\n",
    "saliency = saliency.mean(dim=1) # avg over noisy samples, now shape is [bsz, target tokens, input tokens]\n",
    "\n",
    "# normalization_method, agg_method, post_agg_normalization = \"softmax_over_entire_sequence\", \"sum\", \"none\"\n",
    "normalization_method, agg_method, post_agg_normalization = \"min_max_per_target_token\", \"max\", \"none\"\n",
    "# normalization_method, agg_method, post_agg_normalization = \"log_softmax_per_target_token\", \"max\", \"none\"\n",
    "# normalization_method, agg_method, post_agg_normalization = \"min_max_per_target_token\", \"max\", \"devide_by_sum\"\n",
    "\n",
    "if normalization_method == \"min_max_per_target_token\":\n",
    "    saliency = saliency - saliency.min(dim=-1, keepdims=True).values\n",
    "    saliency = saliency / saliency.max(dim=-1, keepdims=True).values\n",
    "elif normalization_method == \"log_softmax_per_target_token\":\n",
    "    saliency = saliency.log_softmax(dim=-1)\n",
    "elif normalization_method == \"softmax_over_entire_sequence\":\n",
    "    temperature = 1.\n",
    "    saliency = saliency.view(batch_size, -1)\n",
    "    saliency = (saliency / temperature).softmax(dim=-1)\n",
    "    saliency = saliency.view(batch_size, num_target_tokens, num_input_tokens)\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "if agg_method == \"max\":\n",
    "    saliency_agg = saliency.max(dim=1).values\n",
    "elif agg_method == \"sum\":\n",
    "    saliency_agg = saliency.sum(dim=1)\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "if post_agg_normalization == \"none\":\n",
    "    pass\n",
    "elif post_agg_normalization == \"softmax\":\n",
    "    saliency_agg = saliency_agg.softmax(dim=-1)\n",
    "elif post_agg_normalization == \"devide_by_sum\":\n",
    "    saliency_agg = saliency_agg / saliency_agg.sum(dim=1, keepdims=True)\n",
    "else:\n",
    "    raise ValueError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i_example = 2\n",
    "\n",
    "cols = {}\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[i_example])\n",
    "input_tokens = pd.Series(input_tokens).str.replace('Ġ', '_').replace('<s>', 'BOS')\n",
    "cols[\"Input Token MaxPool\"] = input_tokens\n",
    "cols[\"Saliency Agg\"] = saliency_agg[i_example].detach().cpu().numpy()\n",
    "for i_target_token in range(saliency.shape[1]):\n",
    "    target_token = tokenizer.convert_ids_to_tokens([generated_tokens_no_repeats[i_example][i_target_token]])[0].replace('Ġ', '_').replace('<s>', 'BOS')\n",
    "    cols[f\"Input Tokens_{target_token}\"] = input_tokens\n",
    "    cols[f\"Saliency_{target_token}\"] = saliency[i_example][i_target_token].detach().cpu().numpy()\n",
    "\n",
    "saliency_df = pd.DataFrame(cols)\n",
    "cmap = plt.cm.get_cmap(\"coolwarm\")\n",
    "saliency_df = saliency_df.style.background_gradient(cmap=cmap)\n",
    "\n",
    "print(tokenizer.decode(input_ids[i_example]))\n",
    "print()\n",
    "print(tokenizer.decode(generated_tokens_no_repeats[i_example]))\n",
    "saliency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mlskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1388c48c1b848a0052718750b16c870cff087208ab0cafaf53720e4cd74eb1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
